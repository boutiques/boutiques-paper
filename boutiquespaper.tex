% GigaScience template
\documentclass[a4paper,num-refs]{oup-contemporary}

%%% Journal toggle; only specific options recognised.
%%% (Only "gigascience" and "general" are implemented now. Support for other journals is planned.)
\journal{gigascience}

\usepackage{graphicx}
\usepackage{siunitx}
\papercat{Paper}

%%%% Packages %%%%

\usepackage{minted} % Used for JSON highlighting
\usepackage{pdfcomment} % Used for notes and todos
\usepackage{algorithm} % Algorithm float
\usepackage{algpseudocode} % Algorithmic environment
\usepackage{xspace}

%%%% Commands %%%%

\newcommand{\note}[2]{\pdfmargincomment[color=yellow,author=#1,open=true]{#2}}
\newcommand{\todo}[1]{\color{red}TODO: #1\color{black}}
\newcommand{\boutiques}{Boutiques\xspace}
\newcommand{\notimplementedyet}[1]{\color{blue}\emph{#1}\footnote{Still needs to be implemented}\color{black}\xspace}
\algrenewcommand{\algorithmiccomment}[1]{\# \textit{#1}} % so that comments in algorithms are preprended by '#' instead of a small right arrow

\title{Boutiques: a flexible framework for automated application integration in computing platforms}

\begin{document}

\author{Tristan Glatard, Gregory Kiar, Tristan Aumentado-Armstrong,
  Natacha Beck, Pierre Bellec, Remi Bernard, Sorina Camarasu-Pop,
  Fr\'ed\'eric Cervenansky, Samir Das,  Rafael Ferreira da Silva,
  Guillaume Flandin, John Flavin, Pascal Girard, Krzysztof
  J. Gorgolewski, Charles G. Guttmann, Val\'erie Hayot-Sasson,
  Nathaniel Kofalt, Pierre-Olivier Quirion, Pierre Rioux,
  Marc-\'Etienne Rousseau, Gunnar Schaeffer, Alan C. Evans}


\begin{frontmatter}
\maketitle

\begin{abstract}Porting applications to execution platforms such as web
  portals and workflow engines is critical to enable their sharing
  among scientific communities and their efficient execution on
  computing infrastructures. However, application porting remains a
  costly human effort that consists of 1)~installing the application
  on the target infrastructure, 2)~describing the application in a
  format compatible with the execution platform, and 3)~generating
  proper user interfaces. Due to the variety of computing platforms,
  application porting efforts are often replicated several times while
  interoperability would save cost and improve the quality of the ported
  applications. We describe \boutiques, a descriptive framework and
  command line utility that allows for automatic import and exchange of
  applications across platforms. \boutiques adopts a rich, flexible
  application description format which relies on Linux container
  environments Docker and Singularity to install applications
  across platforms in a lightweight manner. \boutiques is currently used
  by three distinct virtual research platforms, and has been used to
  describe dozens of neuroinformatics tools.
\end{abstract}

\begin{keywords}
Keyword1; keyword 2; keyword 3 (Three to ten keywords representing the main content of the article)
\end{keywords}
\end{frontmatter}

\section{Introduction}

\todo{Intro on the relevance of pipeline sharing for reproducible and open science.}

\todo{Check passive voice everywhere.}

Platforms such as web services, portals, scientific gateways, workflow
engines and virtual research environments commonly integrate
third-party applications to facilitate the processing of large
datasets on high-performance computing resources. Such applications,
however, are often repeatedly and manually ported to various platforms
whereas this porting effort would greatly benefit from being
mutualized. Meanwhile, virtual container systems such as Docker\todo{cite} and
Singularity\todo{cite} have emerged to facilitate the sharing of software and
improve the reproducibility of analyses by defining immutable,
reusable execution environments. Nevertheless, the automatic
integration of containerized applications in platforms require formal
annotations of container images, to describe the available command lines.

\boutiques is a system to share command-line applications across
platforms. In \boutiques, a command line is described using a flexible
template comprising the inputs it requires and the outputs it
produces. Inputs may be inter-dependent and passed in configuration
files or directly on the command line. Such formal descriptions,
simply referred to as \emph{descriptors}, link to a container image
where the given application and all its dependencies are
installed. \boutiques descriptors allow for automatic application
integration in platforms, and advanced validation of input values to
prevent errors. \boutiques also provides a simple parallelization
framework that allows parallel applications to concurrently execute
multiple tasks (in the remainder, tasks are called
\emph{invocations}). \boutiques descriptors are intented to be
produced by scientific application developers, stored alongside their
application, indexed by common repositories, and consumed by
execution platforms.  A set of core tools facilitate the construction,
validation, import, execution, and publishing of \boutiques
descriptors.


The remainder of this paper describes the \boutiques system
(Section~\ref{sec:system}) and reports on its adoption by platforms
and applications in the neuroinformatics domain, our primary field of
interest (Section~\ref{sec:results}). It closes on a discussion and
comparison with related systems.

%\note{Tristan}{Vocabulary (check consistency across the paper): tool vs. application}
%\note{Tristan}{Targeted journals: Scientific Data, Giga Science, Frontiers in Neuroinformatics, Future Generation Computer Systems}

\section{System description}
\label{sec:system}

In \boutiques, applications are described with a JSON descriptor that
specifies the command-line template, inputs and outputs. The
descriptor may point to a container where the application and all its
dependencies are installed. It may also contain an invocation schema
used for input validation (this will be created at runtime if it is
not found). At runtime, the execution platform builds the command line
from the descriptor and values entered by the user. The platform runs
the command line on the execution infrastructure, e.g., a server, a
cluster or a cloud, within a container whenever available. To build
and run the command line, the platform may rely on the \boutiques core
tools, in particular the validator and executor, packaged through the
\texttt{bosh} command-line utility.

\subsection{Command-line description}

The core component of the descriptor is a flexible command-line
description represented as a simple string template complying to the
syntax of the \texttt{bash} UNIX shell, the default shell on the major
Linux distributions and OS X. The command-line template may contain
placeholders for input and output values, called value keys. It may
also encompass several commands separated by \texttt{bash} constructs
such as semicolons, pipes (\texttt{|}) or ampersands (\texttt{\&}), to
facilitate the embedding of mundane operations on the command line,
for instance directory creation, input decompression or output
archival.

Here is an example of a typical command-line template:
\begin{verbatim}
          exampleTool_1 [STRING_INPUT] [FILE_INPUT]  | \
                exampleTool_2 [FLAG_INPUT] [NUMBER_INPUT] >> [LOG].txt
\end{verbatim}
The template contains five value keys, identified by square brackets,
that will be replaced by values and file names according to the user
input when the application is executed. Flags will also be added
wherever appropriate, with customizable separators. Value keys have to
be unique but do not have to comply to any particular syntax. Note the
use of the pipe (\texttt{|}) operator to chain applications, and of
the \texttt{>>} operator to redirect the standard output to a file.

\subsection{Input description}

\paragraph{General properties} Inputs must have a name, a unique
identifier, and a type. They may be optional, have a description, a
value key, a flag and flag separator, and a default value. Inputs may
also be ordered lists: in this case, value keys are substituted by the
space-separated list of input values.

\paragraph{Types} Inputs may be of type \texttt{String}, \texttt{Number},
\texttt{Flag} or \texttt{File}. \texttt{File} may also represent a directory.
Types can be restricted to a specific set of values (\texttt{String} 
and \texttt{Number} only), to a specific range (\texttt{Number} only), or to
integers (\texttt{Number} only).

\paragraph{Groups and dependencies} Groups of inputs may be defined
with an identifier, name, and list of input identifiers. Groups may be
used to improve the presentation in a graphical user interface, and to
specify the following constraints among inputs:
(1)~\texttt{mutually-exclusive}: only one member in the group may have
a value; (2)~\texttt{one-is-required}: at least one member in the
group must have a value; (3)~\texttt{all-or-none}: if any of the
members are present all members must be present. Dependencies among
inputs may also be defined regardless of a particular group: an input
may (1)~require a list of inputs and (2)~disable a list of inputs.

Listing~\ref{listing:input-example} shows the definition of an input in the
command line exemplified above. According to this definition and assuming
that the input value entered by the user is 0.3, the string
\texttt{[NUMBER\_INPUT]} will be replaced by \texttt{-n=0.3} on the command
line at execution time.
\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=false,
               xleftmargin=21pt,
               tabsize=4]{js}
{
    "id" : "num_input",
    "name" : "A number input",
    "type" : "Number",
    "value-key" : "[NUMBER_INPUT]",
    "optional" : true,
    "command-line-flag" : "-n",
    "command-line-separator" : "=",
    "minimum" : 0,
    "maximum" : 1,
    "exclusive-minimum" : true,
    "exclusive-maximum" : false
}
\end{minted}
\caption{Example of a \texttt{Number}-type input.} 
\label{listing:input-example}
\end{listing}

\subsection{Output description}

Application outputs are files and directories that need to be
transferred and delivered to the user once the execution is
complete. Outputs need to be specified so that computing platforms can
identify the files that must be saved after the execution and raise
errors if they are not present.

In \boutiques, output files must have a unique identifier, a name, and
a path template that specifies the file or directory name. Path
templates may include input value keys in case output files are named
after the input values. In this case, input values may be stripped
from specific strings, e.g., file extensions, before being substituted
in the path template. Output files may also have a description, a
command-line flag, a flag separator, and a value key in case they
appear on the command line. They may be optional in case the file is
not always produced by the application, for instance when it is
produced only when a particular flag is activated. They may also be
(un-ordered) lists: in this case, the path template must contain a
wildcard (\texttt{*}) matching any string of characters and defining
the pattern used to match the output files in the list.

Listing~\ref{listing:output-example} shows the definition of an output
file in the command line exemplified before. According to this
definition and assuming that the string input value entered by the
user is ``foo.csv'', the string \texttt{[LOG]} will be
replaced by \texttt{log-foo} on the command line at execution time.

\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=false,
               xleftmargin=21pt,
               tabsize=4]{js}

{
    "id" : "logfile",
    "name" : "Log file",
    "description" : "The output log file from the example application",
    "path-template" : "log-[STRING_INPUT]",
    "value-key" : "[LOG]",
    "path-template-stripped-extensions" : [".txt", ".csv"],
    "optional" : false
}
\end{minted}
\caption{Example of an output leveraging \texttt{path-template} search-and-replacement.} 
\label{listing:output-example}
\end{listing}

\subsection{Configuration files}
\label{sec:configuration-files}
A large number of applications rely on configuration files rather than
command-line options to define their input and output parameters. As
the number of parameters increases, command lines rapidly become long
and cumbersome whereas configuration files allow for better structure
and documentation.

Configuration files may be complex though, and specified in any
language.  The \boutiques specification allows application developers
to specify their own template containing input and output value
keys. Configuration files are specific types of output files that must
have a file template in addition to a path template that defines how
they will be named and where they will be written. They may also have
a value key and a flag in case they need to be passed on the command
line. Listing~\ref{listing:configuration-file-example} shows an
example.
\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=false,
               xleftmargin=21pt,
               tabsize=4]{js}
{
    "id": "config_file",
    "name": "Configuration file",
    "type": "Configuration File",
    "value-key": "[CONFIG_FILE]",
    "path-template": "config.txt",
    "file-template": [
        "# This input is hard-coded",
        "stringInput=foo",
        "# This is an input file",
        "fileInput=[FILE-INPUT]",
        "# And here is the result",
        "fileOutput=[OUTPUT-FILE]",
        ""
    ]
}
\end{minted}
\caption{Example of a configuration input file. The file template is defined as
  an array of strings to allow for multi-line strings in JSON.}
\label{listing:configuration-file-example}
\end{listing}

\subsection{Command-line construction}

At runtime, a value is assigned to all the mandatory and some of the
optional inputs. %% Inputs of type ``String'' may contain any string,
%% inputs of type "Number" must contain a string representing a number,
%% inputs of type "File" must contain a string representing a file path
%% (absolute or relative to the execution directory) and inputs of type
%% "Flag" must contain a boolean. 
%% When input is a list, the value contains the concatenation of all the
%%values in the list.
%% gk: I agree with not including the above
Algorithm~\ref{algo:command-line} shows how the command line is
constructed from the descriptor and the values entered by the user. It
substitutes all the value keys in the command line, output path
templates and configuration files, and writes the configuration files.
\begin{algorithm}[h!]
\caption{Command-line construction}
\label{algo:command-line}
\begin{algorithmic}
  \State \Comment{Substitute input value keys in output path templates, configuration files and command line.}
  \For{\texttt{input} in \texttt{inputs}}
  \If{\texttt{input} has a \texttt{value-key}}
  \For{\texttt{output} in \texttt{outputs}}
  \State \texttt{stripped\_value} = \texttt{input\_value}
  \If{\texttt{input} type is \texttt{File}}
  \State In \texttt{stripped\_value}, remove  all elements in \texttt{path-template-stripped-extensions}.
  \EndIf
  \State In \texttt{path-template}, replace all occurrences of \texttt{value-key} by \texttt{stripped\_value}.
  \If{\texttt{output} has a \texttt{file-template}}
  \State In any line of \texttt{file-template}, replace all occurrences of \texttt{value-key} by \texttt{stripped\_value}.
  \EndIf
  \EndFor
  \State Prepend \texttt{command-line-flag} and \texttt{command-line-flag-separator} to \texttt{input\_value}.
  \State In \texttt{command-line}, replace all occurrences of \texttt{value-key} by \texttt{input\_value}.
  \EndIf
  \EndFor

  \State \Comment{Substitute output value keys in configuration files and command line.}
  \For{\texttt{output} in \texttt{outputs}}
  \If{\texttt{output} has a \texttt{value-key}}
  \For{\texttt{output} in \texttt{outputs}}
  \If{\texttt{output} has a \texttt{file-template}}
  \State In any line of \texttt{file-template}, replace all occurrences of \texttt{value-key} by \texttt{path-template}.
  \State \Comment{Input \texttt{value-key} have been substituted in \texttt{path-template} previously.}
  \EndIf
  \EndFor
  \State Prepend \texttt{command-line-flag} and \texttt{command-line-flag-separator} to \texttt{path-template}.
  \State In \texttt{command-line}, replace all occurrences of \texttt{value-key} by \texttt{path-template}.
  \EndIf
  \EndFor

  \State \Comment{Write all configuration files.}
  \For{\texttt{output} in \texttt{outputs}}
  \If{\texttt{output} has a \texttt{file-template}}
  \State Write \texttt{file-template} in \texttt{path-template}
  \State \Comment{Value keys have been substituted in \texttt{file-template} previously.}
  \EndIf
  \EndFor

\end{algorithmic}
\end{algorithm}

\subsection{Invocation schema}
\label{sec:invocation-schema}

Rigorous input validation is an important motivation for
\boutiques. For this purpose, \boutiques relies on an
application-specific JSON schema\todo{cite}, called \emph{invocation schema}, to
specify the input values accepted by an application. Platforms can
rely on invocation schemas to validate inputs using
any JSON validator, without having to develop specific code.

Invocation schemas, however, are complex JSON objects. Basically, they
must represent the properties described above in a formal way,
including dependencies between
inputs. Listing~\ref{listing:invocation-schema-example} shows an
example of how dependencies between mutually exclusive parameters are
defined in the invocation schema. To relieve application developers
from the burden of having to write invocation schemas, invocation
schemas can be generated automatically by the \texttt{bosh}
command-line utility. The invocation schema is stored as an optional
property of the \boutiques descriptor.

%% \notimplementedyet{The invocation schema also has a
%%  \texttt{task-dependencies} array that allows specifying that the new
%%  invocation may start only when previously-submitted invocations have
%%  completed. This property is used in the parallelization model
%%  described hereafter.}

\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=false,
               xleftmargin=21pt,
               tabsize=4]{js}
{
  "dependencies" : {
      "num_input" : {
         "properties" : {
            "str_input" : {
               "not" : {}
            }
         }
      },
      "str_input" : {
         "properties" : {
            "num_input" : {
               "not" : {}
            }
         }
      }
   }
}
\end{minted}
\caption{Excerpt from invocation schema showing dependencies between
  two mutually exclusive parameters \texttt{num\_input} and
  \texttt{str\_input}.}
\label{listing:invocation-schema-example}
\end{listing}

%% \subsection{Parallelization support}
%% \label{sec:parallelization}
%% \boutiques supports parallelism by allowing applications to submit new
%% invocations through \notimplementedyet{the following protocol}:
%% \begin{enumerate}
%% \item Applications write a JSON object complying to the invocation
%%   schema in their execution directory.
%% \item The platform reads the JSON object, creates the corresponding
%%   invocation, and writes back the invocation id in the same directory.
%% % The platform could also support status requests.
%% \end{enumerate}
%% Several invocations can be submitted at once, possibly using the
%% \texttt{task-dependencies} array to specify dependencies among
%% them. This mechanism is implemented in the Boutiques schema using a
%% single property, \texttt{can-submit-tasks}.

%% Although very simple, this model allows wrapping complete
%% workflows. Workflows are wrapped as any other application, and they
%% can be executed using any engine provided that it is installed in the
%% container image referred in the application descriptor. 

\subsection{Workflow support}

\boutiques does not specify a particular language to compose
descriptors in a workflow, due to the large amount of specialized
frameworks to do so. However, workflows can be both composed from and
described as \boutiques descriptors: workflow engines can leverage the
\texttt{bosh} tools to call \boutiques tools from their descriptors;
in turn, workflows can be described as \boutiques descriptors provided
that the execution engine is containerized. Such a ``task
encapsulation'' model allows for a scalable and reliable execution of
workflows expressed in a variety of languages, as detailed in
~\cite{glatard2017software}.

\subsection{Containers}

Applications are installed in a container image complying to the
Docker, Singularity or rootfs format. We intentionally support
multiple container formats as we anticipate that they will be used for
different purposes. For instance, Docker is well suited for developers
and users who manipulate applications on their local workstations or the
cloud. It is well documented, maintained and it has a rich ecosystem of
tools to help build and run containers on most operating systems. Singularity
is more suited for users and platforms that need to run applications
on shared computing clusters. Bridges exist among these containers
formats to convert container images across frameworks. For instance, a
platform dedicated to high-performance computing may accept descriptors
referring to Docker containers to facilitate application integration
by developers, and internally convert container images to Singularity
to run applications efficiently on clusters.

Container images are defined from their URL (rootfs) or image name in
a Docker or Singularity index. Descriptors and container images may
specify a working directory where the application has to be run,
descriptors may contain a boolean indicating if the image has an entry
point, and a hash may also be reported to accurately identify
container images and detect updates.

Containers were adopted because they allow for an automated and
lightweight integration of application implementations in
platforms. They are extremely useful to improve the reproducibility of
analyses as variations in the software environment may have an
important impact on the computed results. They also have limitations,
in particular they do not specify the hardware architecture required
to execute an application, which can be an issue in some cases.

%%Containers are not mandatory though. If an application descriptor does
%%not contain any reference to a container, the platform will assume
%%that the application is pre-installed on the execution infrastructure. 
%% gk: I believe we only support containers now, correct?

\subsection{Resource requirements}

\boutiques descriptors may contain requirements regarding the number
of CPU cores or nodes, the amount of RAM or disk storage, and the
total walltime expected for a typical execution of the
application. Such properties are called ``suggested resources'' as we
are well aware that the actual resource requirements usually depend on
the input data, parameters and hardware infrastructure.


\subsection{Custom properties}

Custom properties may be added to the Boutiques specification without
restriction. Custom properties are corralled in a specific JSON object
to facilitate validation. They may be useful to implement
platform-specific features but they should be used with care to avoid
making applications dependent on a particular platform or replacing
existing functionality already represented in the \boutiques schema.

\subsection{Core tools} 

\boutiques is available on Python through the PyPi package repository
as ``\texttt{boutiques}''. The \boutiques package exposes a
command-line utility to the user, \texttt{bosh}, which contains
entrypoints for all core functions within \boutiques. There is
help-text which should guide users through interacting with the
\texttt{bosh} utility. The core tools provided by \boutiques are, the:
validator, executor (for both launch and simulation), invocation
schema handler, importer, and publisher. The tools exposed through the
command-line API are also available consistently through the Python
interface by importing the \boutiques package. Though not a component
of the \boutiques tool chain, a Jupyter Notebook tutorial exists to
facilitate new users getting started with \boutiques.

\paragraph{Validator} The \boutiques validator checks conformance of JSON
descriptors to the \boutiques schema using a basic JSON validator. It
also performs the following checks that cannot be easily implemented
in JSON schema: value keys are unique among inputs, input and output
identifiers are unique, input and output value keys are all included
in the command line, identifiers with the same value key are mutually
exclusive, value keys are not contained within each other (which would
puzzle substitution), output path templates are unique (to avoid
results to overwrite each other), inputs of type Flag have a
command-line flag, are optional and are not lists, the default value
of restricted types is part of the restriction, an input cannot both
require and disable another input, required inputs cannot require or
disable other parameters, group member identifiers must correspond to
existing inputs and cannot appear in different groups, mutually
exclusive groups cannot have members requiring other members,
one-is-required groups should never have required members, and
all-or-none group members must not be required.


\paragraph{Executor} The executor has two modes of operation: \emph{launch}, and
\emph{simulate}. The simulate mode can generate hypothetical command lines from
random values given the descriptor (and corresponding invocation schema) for debugging
purposes, or display the command that would be executed given a provided valid
invocation. The launch mode can execute command line from a \boutiques descriptor and
a set of input values represented in JSON file complying with the invocation schema.
It runs the command in a container provided that the required framework (e.g. Docker)
is already installed. The executor can be used by application users to run
applications locally, or by platforms to generate command lines to be run on the
execution infrastructure.

\paragraph{Invocation Schema Handler} The invocation schema handler can create an
invocation schema (see Section~\ref{sec:invocation-schema}) from a \boutiques descriptor
and validate input data against it using a regular JSON validator. It can be used to add
invocation schemas to existing descriptors. It is used by the executor if no invocation
schema is present in the \boutiques descriptor being deployed.

\paragraph{Importer} The importer takes \boutiques descriptors from older versions and
updates them to be compliant with the most recent version of the schema. This tool can
also create descriptors from selected application collections, such as BIDS
apps~\cite{gorgolewski2017bids}.

\paragraph{Publisher} As \boutiques has primarily been adopted in the neuroinformatics
community, the publisher gets a further description (such as author, website, etc.) of
the described tool, and adds an index to it on
NeuroLinks\footnote{\url{https://brainhack101.github.io/neurolinks}}, a repository
containing links to useful resources and tools. This enables others to find and use the
descriptor for your tool easily. This functionality could be extended to
new repositories for other domains, such as Bioconductor for bioinformatics.

\section{Results}
\label{sec:results}

\subsection{Supported platforms}

\boutiques application import and execution is currently supported by several
platforms which aid in execution and tool management. These are enumerated
below.

\subsubsection{CBRAIN}

CBRAIN~\cite{SHER-14} is a web platform that can process data
distributed into multiple storage locations on computing clusters and
clouds. CBRAIN offers transparent access to remote data sources,
distributed computing sites, and an array of processing and
visualization tools within a controlled, secure environment.  The
CBRAIN service deployed at the Montreal Neurological Institute relies
on the infrastructure provided by Compute Canada~\cite{das2016mni}. It
currently provides 500+ collaborators in 22 countries with web access
to several systems, including 6 clusters of the Compute Canada
high-performance computing infrastructure (totaling more than 100,000
computing cores and 40PB of disk storage) and Amazon EC2. CBRAIN
transiently stores about 10 million files representing over 50TB
distributed in 42 servers. 51 public data processing applications are
integrated and over 340,000 processing batches have been submitted since 2010.

Applications in CBRAIN are integrated as Ruby classes that create web
forms, validate parameters and run command lines on computing
resources. \boutiques is supported through a set of templates that
generate such classes from the application descriptor. Two application
integration modes are available:
\begin{enumerate}
  \item The descriptor is stored in a CBRAIN plugin and the Ruby
    classes are generated on-the-fly when CBRAIN starts. This mode
    allows CBRAIN developers update all \boutiques applications at
    once by editing the templates. However, it does not allow for
    customization beyond the \boutiques schema. To provide more
    flexibility, we added a custom property
    (\texttt{cbrain:inherits-from-class}) to the \boutiques descriptor
    to define the Ruby class that should be used as parent class for
    the application.
  \item Ruby classes are generated from descriptors
    through an offline process and integrated in CBRAIN as any other
    application. This mode allows developers to customize applications
    by editing the generated Ruby classes, but the resulting
    applications are difficult to maintain in the long term, in
    particular when the descriptors are updated.
\end{enumerate}

We also extended CBRAIN to enable the parallelization of workflows
wrapped as \boutiques descriptors. Applications with the
\texttt{cbrain:can-submit-new-tasks} custom property may submit sub-tasks by
creating \boutiques invocations in their working directory. CBRAIN
periodically scans working directories, submits the requested
sub-tasks, and writes back the invocation id in the same
directory. This parallelization model is simple, the application only
needs to write \boutiques invocations and communication happens
through the file system, and it is also powerful as it enables the
parallelization of complex workflows such as the FSL and Niak ones
described later. 

We also introduced a new list mechanism in CBRAIN to facilitate the
iteration of \boutiques applications on large sets of files. CBRAIN
lists are specific files that contain references to other CBRAIN
files. When a list is passed to a \boutiques application, the elements
in the list are either concatenated in a single command line (when the
corresponding \boutiques input is a list), or a new command line is
generated for every element in the list (when the input is not a
list). Supporting lists as a specific CBRAIN file type allows for
improved validation. For instance, lists that contain references to
non-existent or deleted files can be detected. It also allows users to
edit lists using their own tools such as scripts or spreadsheet
applications.

\subsubsection{Flywheel}
%% gk: shall we check with them, and if nothing, remove?
\todo{Gunnar, Nathanael, John}

\subsubsection{Pegasus}
%% gk: to what extent does this exist?
\todo{Rafael}

\subsubsection{SPINE}
%% gk: shall we check with them, and if nothing, remove?
\todo{Charles}

\subsubsection{VIP}

The Virtual Imaging Platform (VIP)~\cite{GLAT-13} is a web portal for
medical simulation and image data analysis. VIP makes applications
available as services, and connects them to the biomed Virtual
Organization (VO) in the European Grid
Infrastructure\footnote{\url{https://www.egi.eu}}. The biomed VO
interconnects approximately 65 computing sites world-wide and provides
access to 130 computing clusters and 5~PB of storage.  The VIP service
is deployed at the Creatis
laboratory\footnote{\url{https://www.creatis.insa-lyon.fr}} in Lyon
and it uses the DIRAC French national
service\footnote{\url{https://dirac.france-grilles.fr}} to execute
jobs on EGI grid and cloud resources.  As of October 2017, VIP counts
more than 1000 registered users and a growing number of available
applications.
%% gk: I guessed 50 more in the last year, since 1000 is a nice number, is this accurate?

Until recently, applications were manually integrated in VIP as
workflows written in the Gwendia~\cite{MONT-09} language and executed
with the MOTEUR~\cite{GLAT-08} engine.  As of today, Boutiques is
supported through a "Boutiques application importer" tool, which
parses the JSON descriptor and automatically generates the
corresponding application workflow and the wrapper script that
handles, among other things, the execution of the command line.
Application workflows enable (1) iterations on input lists, (2) the
generation of parallel tasks, and (3) the concatenation of multiple
tools. For examples, the applications used in the MICCAI challenges
described below required workflows to evaluate results using the
metrics defined by the challenge. Tool concatenation is 
handled at the importer level based on pre-defined workflow
templates.

\subsection{Ported applications}

% cbrain-plugins-neuro

%%Table~\ref{table:applications} summarizes the applications described
%%with \boutiques and the main features used.

Dozens of neuroinformatics tools were ported to CBRAIN or VIP using
\boutiques. The main ones are described below. Several \boutiques
descriptors were published in
Neurolinks\footnote{\url{https://brainhack101.github.io/neurolinks}}
through the \texttt{bosh} publisher.



%% \paragraph{Nipype} We instrumented the Nipype workflow engine~\cite{gorgolewski2011nipype} 
%% to export the applications described in its internal Python
%% description language to \boutiques. \todo{Update nipype\_cmd to
%%   support the latest \boutiques schema and complete this paragraph.}

\subsubsection{Anatomical MRI}

\paragraph{FSL} Several tools from the FMRIB Software
library (FSL) were ported to CBRAIN using \boutiques: BET, fsl\_anat, FAST,
FIRST and probtrackx2. Descriptors are available on Neurolinks.

\paragraph{Anima}
The animaN4BiasCorrection tool from the
Anima\footnote{\url{https://github.com/Inria-Visages/Anima-Public/wiki}} project was also made available 
in VIP. The import process was straightforward. \todo{Sorina: reference?}

\subsubsection{Functional MRI}

\paragraph{Niak} The Niak pre-processing pipeline, executed with the Pipeline System
for Octave and Matlab (PSOM)~\cite{bellec2012pipeline}, was integrated
in CBRAIN through \boutiques. The integration uses the parallelization
mechanism described earlier so that even
the invocations processing a single subject can be parallelized. It also
allows CBRAIN to leverage the efficient agent model used in PSOM, as
described in~\cite{GLAT-16}. The integration required some work in
PSOM to facilitate its invocation as a non-interactive command-line
application. The resulting CBRAIN plugin is available at
\url{https://github.com/SIMEXP/cbrain-plugins-psom}. It can
be used in other platforms that support the parallelization
property. Descriptors are available on Neurolinks.

\paragraph{GinFizz}
Ginfizz, a FMRI pipeline allowing to execute multiple 
preprocessing steps and basic analyses such as: (i) realignment (motion registration and correction),
(ii) slice timing (temporal interpolation to correct for the delay of acquisition between slices), 
(iii) coregistration (registering functional and structural data), (iv) normalization (registering 
of functional and anatomical data to a match a template that has to be specified), and (iv)
smoothing (spatial filtering to increase Signal to Noise ratio). The use of Docker containers
(\url{https://hub.docker.com/r/fliiam/ginfizz} based on \url{https://hub.docker.com/r/miykael/nipype_level2}) 
greatly simplifies its deployment.
\todo{Sorina: reference?}

\subsubsection{Diffusion imaging}

\paragraph{MRTrix3}
A few tools from the MRtrix3\footnote{\url{http://www.mrtrix.org/}}
package for diffusion MRI processing were also made available in VIP,
as well as a script developed at Creatis, which combines MRtrix3 and
FSL tools.

\paragraph{ndmg} The NeuroData MRI to Graphs one-click connectome estimation
pipeline~\cite{kiar2017comprehensive}, developed in Python and leveraging FSL, was
exported to \boutiques and is available at \url{https://github.com/neurodata/boutiques-tools}.
The ndmg pipeline is deployed in CBRAIN via its \boutiques descriptor, and is available
both through Docker and Singularity container environments.

\subsubsection{Image simulation}

\paragraph{CreaPhase}
A phase contrast simulator called CreaPhase, developed at Creatis, was
integrated in VIP through \boutiques. The CreaPhase inputs had
certain particularities (some nedeed to be enclosed in simple quotes,
other were vectors of variable size enclosed in brackets) that
required the post-processing of the wrapper script generated by the
Boutiques importer. \todo{Sorina: reference?}

\paragraph{ODIN} The Odin MRI simulator\footnote{\url{http://od1n.sourceforge.net}} was imported 
in VIP through \boutiques. Since Odin may require important amounts of
computing resources (the common use-case consists in iterating on a
large number of inputs files), Odin is executed by VIP on the EGI grid
resources, which currently do not support Docker. Consequently, the
Odin executable was exported from the Docker image and the Odin
wrapper script was modified accordingly.

\subsubsection{General-purpose toolboxes}

\paragraph{BIDS apps} BIDS apps, a recent effort for the adoption of the Brain Imaging Data Structure (BIDS)
in common neuroimaging pipelines, require a standardized set of input
and output parameters~\cite{gorgolewski2017bids}. We developed a tool
as part of the \boutiques importer to generate a descriptor for any
such BIDS app. We validated this tool by importing 
BIDS apps containing the Statistical Parameter Mapping toolbox
(SPM)~\cite{penny2011statistical} and the ndmg pipeline mentioned
above. Descriptors are available on Neurolinks.

\paragraph{2016 MICCAI challenges}
We used \boutiques to integrate 23 pipelines in the VIP platform in
the context of two challenges organized by the MICCAI conference in
2016, related to the segmentation of multiple-sclerosis lesions in MR
images (MSSEG
challenge\footnote{\url{https://portal.fli-iam.irisa.fr/msseg-challenge/overview}})
and of tumor volumes in PET images (PETSEG
challenge\footnote{\url{https://portal.fli-iam.irisa.fr/petseg-challenge/overview}}). The
pipelines were ported to VIP and executed on 205 subjects in a few
weeks only, which would not have been possible without
\boutiques. Some pipelines had to be adjusted manually once ported to
the platform for the following reasons:
\begin{enumerate}
\item A pipeline
  required a GPU, which we enabled through the
  \texttt{nvidia-docker}\footnote{\url{https://github.com/NVIDIA/nvidia-docker}}
  tool not supported in Boutiques although it could be a possible extension.
\item A pipeline required more than 10 GB of data dependencies (atlas
  data) which exceeded the maximum size allowed for Docker containers
  in our setup. We solved the issue by installing the data in a
  directory of the host server that we mounted in the container. Such
  platform-specific configurations could not possibly be supported by
  \boutiques.
\item A pipeline wrote more than 10 GB of intermediate data in a
  temporary directory of the container located on a 2 GB partition. We
  solved the issue by mounting a host directory in the temporary directory. 
\end{enumerate}
The pipelines did not use much of the expressiveness features offered
by \boutiques since the instructions given to the challengers required
that all the pipeline parameters were hardcoded, to facilitate
execution. Two custom properties (\texttt{vip:miccai-challenger-email}
and \texttt{vip:miccai-challenge-team-name}) were added to the
\boutiques descriptor to help post-process results in the specific
context of MICCAI challenges.


\section{Discussion}
With \boutiques, developers can port their applications once and execute
them in several platforms. \boutiques removes the technological dependency to a
particular platform and facilitates migration.  Although the
motivating use cases were taken from neuroinformatics, our primary
field of interest, nothing prevents the system from being used in a
variety of other domains.

\subsection{\boutiques descriptor}

The \boutiques descriptor specification allows describing a wide range
of applications, but it is also getting increasingly complex through
additions such as invocation schemas and dependencies among
inputs. Extending the \boutiques descriptor has two main goals. First,
accurately describing applications improves validation: incorrect input
values and execution results are more precisely detected when the
application descriptor is comprehensive, which is essential to improve
the reliability and user-friendliness of execution
platforms. Secondly, a rich descriptor schema reduces the need for
custom wrappers to integrate applications as even convoluted command
lines can be described: this is particularly interesting for
applications installed in containers since adding a wrapper to the
execution stack requires an update in the container image.

Nonetheless, a complex descriptor schema has a cost for application
developers and platforms, which we address as follows. For developers,
we maintain the set of mandatory descriptor properties as small as
possible so that simple applications can be described in a few lines
only (see Listing~\ref{listing:minimal}). For platforms, we aim at
supporting as many features as possible in the executor so that
only the following steps need to be implemented to support \boutiques
in a platform, regardless of the complexity of the descriptor:
\begin{enumerate}
  \item Input entry: generate the interface to allow users to enter
    input values.
  \item Input validation: create a JSON object from the interface,
    validate it against the invocation schema.
  \item Execution: generate and run the command line from the local
    executor and input JSON object.
  \item Output delivery: from the descriptor, identify the output files
    and deliver them to the user.
\end{enumerate}
In particular, advanced validation features such as dependencies
between inputs are embedded in the regular JSON validation performed
at step 2, without requiring the platform to support the related
descriptor properties.
\begin{listing}
\begin{minted}[frame=single,
               framesep=3mm,
               linenos=false,
               xleftmargin=21pt,
               tabsize=4]{js}
{
    "name": "echo",
    "tool-version": "1.0",
    "description": "A simple script to test output files",
    "command-line": "echo [PARAM] > output.txt",
    "schema-version": "0.4",
    "inputs": [{
	"id": "param",
	"name": "Parameter",
        "value-key": "[INPUT_FILE]",
	"type": "File"
    }],
    "output-files": [{
	"id": "output_file",
	"name": "Output file",
	"path-template": "output.txt"
    }]

}
\end{minted}
\caption{A minimal \boutiques descriptor.} 
\label{listing:minimal}
\end{listing}

\subsection{Workflow support}

The parallelization framework described in~\ref{sec:parallelization}
supports workflows by considering them as both applications and
platforms, which we think is a powerful but underexploited
model. Workflows are applications because they are invoked like any
other script or executable, possibly through a command line. Workflows
are also platforms because they may invoke other applications that
might themselves be described in \boutiques. This model is powerful
because it shields the \boutiques specification from specific workflow
constructs and allows a wide range of workflow engines to be described
uniformly. 

\todo{CBRAIN's custom property might be standardized.}

\subsection{Reproducibility}

\boutiques helps reproduce\footnote{in Peng's
  sense~\cite{peng2011reproducible}} executions through containers and
formal command-line descriptions. With \boutiques, complete sets of
applications could be easily migrated across execution platforms,
including high-performance computing platforms and individual laptops,
to reproduce analyses. The \boutiques descriptor describes the
parameters and implementation of the application, and the invocation
schema describes the parameter values.

However, reproducibility is a large problem that \boutiques only
partially addresses. At the command-line execution level, containers
help freeze a large fraction of the software ecosystem but they do not
shield against discrepancies arising from different Linux kernel
versions or hardware platforms. For instance, containers may not
execute consistently on different x86\_64 CPUs (e.g. Intel and AMD)
when software compiled with architecture-specific flags is involved
(see for instance GCC's \texttt{-march} flag).

In addition, important runtime parameters, for instance related to
multi-threading or available resources (storage, RAM), may be set by
the execution platform without being specified in the \boutiques
descriptor. Such runtime parameters may impact reproducibility in some
cases. To properly cover this issue, \boutiques descriptors should be
complemented by a provenance framework that captures a detailed trace
of the execution.

At a higher level, reproducibility is impacted by factors
such as data sharing~\cite{poldrack2014making} which are out of the
scope of \boutiques.

\subsection{Application types}

So far, \boutiques has focused on the description of non-interactive
command-line applications. While such applications cover a large
fractions of the tools involved in scientific data processing, other
types of programs exist such as web services, interactive tools, and
applications with a graphical user interface (GUI).  Such application
classes could be described in \boutiques through a command-line
mapping. For instance, web services may be wrapped as command-line
applications using tools such as \texttt{curl} or
\texttt{wget}. Interactive applications may also be transformed to
non-interactive ones through configuration files. Finally, nothing
prevents a \boutiques tool from popping up a window for a user to
provide input through a GUI. This should, however, be specified
as an extension to the descriptor since most platforms would not support
this feature by default. Graphical output produced by applications
executed in containers should also be treated specifically.

\subsection{Limitations}

A few limitations remain that should be addressed in the
future. First, \boutiques moves the application porting bottleneck
from integration to validation. Using \boutiques, functions can be
automatically exported from frameworks such as Nipype and SPM,
creating hundreds of richly described applications potentially usable
by end-users. However, it remains unclear how to validate that these
applications actually execute as expected beyond executing packaged tests
distributed alongside a given tool. A \boutiques-specific testing
framework could be designed and potentially fed by existing frameworks,
to address this issue.

Another limitation is related to the security of containerized
applications. Since containers are usually controlled by application
developers rather than platforms, which is a good thing to reduce
application porting bottlenecks, nothing prevents developers to embed
malicious code in their container at any stage of the process,
possibly after a platform administrator inspected the
container. Containers are currently bulky file archives that are
cumbersome to inspect. Tools need to be developed to allow for an
easier characterization of container contents, for instance through
comparison digests with respect to validated base images. Singularity
containers have reduced security risks as compared to Docker containers,
but the issue of content transparency is still not avoided.

% How to build modular container images for workflows.

\section{Related work}

\todo{BIDS apps}

The Common Workflow Language\footnote{\url{http://www.commonwl.org}}
is the piece of related work most closely related to
\boutiques. Several other application description frameworks have been
developed but they do not support containers.

\subsection{Common Workflow Language}

The Command Line Tool Description of the Common Workflow Language
(CWL) overlaps with the \boutiques descriptor. According to the history
of the CWL and \boutiques Github repositories, CWL started 6 months
before \boutiques (September 2014 vs. May 2015). The differences
highlighted below were identified from version 1.0 of the CWL Command
Line Tool
Description\footnote{\url{http://www.commonwl.org/v1.0/CommandLineTool.html}}.

\subsubsection{Conceptual differences}

The following differences are conceptual in the sense that they may
not be easily addressed by CWL or \boutiques developers without deeply
refactoring the frameworks.

First, CWL has a workflow language whereas \boutiques does not. In
\boutiques, workflows are integrated as any other applications, except
that they may submit other invocations to enable workflow
parallelism. This fundamental difference has consequences on the
complexity of CWL application descriptions and on the possibility to
reuse existing workflows in \boutiques. The adoption of ontologies in
CWL may also be another consequence (see below).

CWL has a formal command-line definition while \boutiques is more
flexible. CWL specifies command lines using an array containing an
executable and a set of arguments whereas \boutiques only uses a
string template. \boutiques' template approach may create issues in
some cases, but it also allows developers to add simple operations to
an application without having to write a specific wrapper. For
instance, a \boutiques command line may easily include input
decompression using, e.g., the tar command in addition to the main
tool command. On the contrary, CWL applications may only have a single
executable. Importantly, \boutiques' template system allows supporting
configuration files (see Section~\ref{sec:configuration-files}).

CWL uses ontologies while \boutiques does not. Ontologies allow for
richer definitions but they also have an overhead. The main
consequences are the following:
\begin{itemize}
\item CWL uses a specific framework for
validation, called SALAD (Semantic Annotations for Linked Avro Data)
whereas \boutiques uses plain JSON schema. The main goal of SALAD is to
allow ``working with complex data structures and document formats, such
as schemas, object references, and namespaces''. \boutiques only relies
on the basic types required to describe and validate a command line
syntactically. While the use of SALAD certainly allows for
higher-level validation, and may simplify the composition and
validation of complex workflows, it also introduces a substantial
overhead in the specification, and platforms have to use the validator
provided by CWL. On the contrary, a regular JSON validator can be used in \boutiques.
\item  CWL has a rich set of types whereas \boutiques only has simple
types. Again, this may be seen as a feature or as an overhead
depending on the context. \boutiques tries to limit the complexity of
the specification to facilitate its support by platforms where tools
will be integrated.
\end{itemize}

\subsubsection{Major differences}

The following differences are major but they may be addressed by the
CWL and \boutiques developers as they do not undermine the application
description model.

\begin{itemize}
\item CWL tools have to write in a specific set of directories called
``designated output directory'', ``designated temporary directory''
and ``system temporary directory''. Tools are informed of the location
of such directories through environment variables. Having to write in
specific directories is problematic because tools have to be modified
to enable that. In \boutiques, the path of output files is defined
using a dedicated property.
\item CWL types are richer, not only
semantically but also syntactically. For instance, files have
properties for basename, dirname, location, path, checksum, etc.
\item \boutiques supports various types of containers (Docker, Singularity,
rootfs) while CWL supports only Docker. Both tools have rich requirements:
for instance, they may include RAM, disk usage, and walltime estimate; 
whereas \boutiques also supports number of requested CPU cores. CWL has hints, i.e.,
recommendations that only lead to warnings when not respected.
\item In \boutiques, dependencies can be defined among inputs, e.g., to
specify that an input may be used only when a particular flag is
activated. This is a very useful feature to improve validation, in
particular when tools have a lot of options.
\item In \boutiques, named
groups of inputs can be defined, which improves the presentation of
long parameter lists for the user and enables the definition of more
constraints within groups (e.g. mutually exclusive inputs).
\end{itemize}

\subsubsection{Other minor differences}

The \boutiques and CWL schemas have other minor differences that we
summarized in a public
spreadsheet\footnote{\url{https://docs.google.com/spreadsheets/d/1eLhpxzaaMPCIUFmV5Trs-WtaMpBO-c6QDQsmhwP7QIE/edit\#gid=0}}.

\subsection{Other frameworks}

Several frameworks have been created to facilitate the
integration of command-line applications in platforms. In
neuroinformatics, many platforms define a formal interface to embed
command-line applications. Among them, the Common
Toolkit\footnote{\url{http://www.commontk.org}} interoperates with
several platforms such as 3D Slicer~\cite{pieper20043d},
NiftyView~\cite{Craddock2016}, GIMIAS~\cite{larrabide2009gimias},
MedInria~\cite{larrabide2009gimias}, MeVisLab~\cite{heckel2009object}
and MITK workbench~\cite{nolden2013medical}. The framework, however,
remains tightly bound to the Common Toolkit's C++ implementation which
limits its adoption, e.g., in web platforms \todo{Fr\'ed\'eric, maybe
  you have more to say on this}.

In the distributed computing community, systems were also proposed to
facilitate the embedding of applications in platforms. The Grid
Execution Management for Legacy Code Architecture
(GEMLCA~\cite{delaitre2005gemlca}) was used to wrap applications in
grid computing systems. Interestingly, it has been used to embed
workflow engines in the SHIWA
platform~\cite{terstyanszky2014enabling}, in a similar but different
way than proposed by \boutiques (see Section~\ref{sec:parallelization}).

The recent advent of virtual containers requires a new generation of
application description frameworks that are independent from any
programming language and that expose a rich set of properties to
describe command lines, as intended by \boutiques.

\section{Conclusion}

\boutiques is available at \url{https://github.com/boutiques}. We
welcome feedback, issue reporting and pull requests. \boutiques adopts
a bottom-up approach where new features are progressively added based
on feedback from applications and platforms. Beyond the technicalities
discussed in this paper, the availability of a solid core of
applications and platforms in the framework is key to its success,
which we plan to continuously enhance.

\section{Acknowledgments}

\todo{FLI}

\bibliography{biblio}

\end{document}
